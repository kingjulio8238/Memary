{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_35 = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "llm_4 = OpenAI(model=\"gpt-4-0613\")\n",
    "# base_agent = ReActAgent.from_tools(query_engine_tools, llm=llm_35, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "\n",
    "# gpt-4\n",
    "step_decompose_transform = StepDecomposeQueryTransform(llm=llm_4, verbose=True)\n",
    "\n",
    "# gpt-3\n",
    "step_decompose_transform_gpt3 = StepDecomposeQueryTransform(\n",
    "    llm=llm_35, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./query-decomposition/data/filler\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, llm=llm_35, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_summary = \"Used to answer questions about the teacher, the students, and the classroom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# limit the context window artifically to test refine process\n",
    "Settings.context_window = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "from llama_index.core.query_engine import MultiStepQueryEngine\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm_4)\n",
    "query_engine = MultiStepQueryEngine(\n",
    "    query_engine=query_engine,\n",
    "    query_transform=step_decompose_transform,\n",
    "    index_summary=index_summary,\n",
    ")\n",
    "\n",
    "query_engine.callback_manager = callback_manager\n",
    "query_engine.callback_manager.add_handler(finetuning_handler)\n",
    "query_engine.callback_manager.start_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = './query-decomposition/cv_loc_enhanced_questions.json'\n",
    "with open(file_path, 'r') as file_loc:\n",
    "    data = json.load(file_loc)\n",
    "\n",
    "new_questions = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_questions(questions, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        for question in questions:\n",
    "            f.write(question + \"\\n\")\n",
    "            \n",
    "def load_questions(path):\n",
    "    questions = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            questions.append(line.strip())\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split up into cv and loc questions, also train on each possible set of eval/test split\n",
    "train_questions, eval_questions = new_questions[:60], new_questions[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_questions(train_questions, \"train_questions_10q.txt\")\n",
    "save_questions(eval_questions, \"eval_questions_10q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = load_questions(\"train_questions_10q.txt\")\n",
    "eval_questions = load_questions(\"eval_questions_10q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"documents\",\n",
    "            description=\"Query engine that decomposes one complex query into its parts\",\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4-0613\")\n",
    "gpt4_agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change Index of train_questions[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Question: Is the teacher standing in front of the classroom while giving a lecture or conducting an activity?\n",
      "\u001b[1;3;33m> Current query: Is the teacher standing in front of the classroom while giving a lecture or conducting an activity?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: Is the teacher standing in front of the classroom?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Is the teacher standing in front of the classroom while giving a lecture or conducting an activity?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: Is the teacher giving a lecture or conducting an activity?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Is the teacher standing in front of the classroom while giving a lecture or conducting an activity?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[0] Agent Response: The context does not provide information about whether the teacher is standing in front of the classroom while giving a lecture or conducting an activity.\n",
      "[1] Question: How does the classroom arrangement affect the teacher's ability to interact with students at their desks?\n",
      "\u001b[1;3;33m> Current query: How does the classroom arrangement affect the teacher's ability to interact with students at their desks?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What is the current arrangement of the classroom?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: How does the classroom arrangement affect the teacher's ability to interact with students at their desks?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are some common classroom arrangements?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: How does the classroom arrangement affect the teacher's ability to interact with students at their desks?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[1] Agent Response: The classroom arrangement can impact the teacher's ability to interact with students at their desks based on factors such as the layout of the desks, the distance between the teacher and students, and the visibility of all students from the teacher's position.\n",
      "[2] Question: Is the student writing an answer on the whiteboard as part of a group activity or individual response?\n",
      "\u001b[1;3;33m> Current query: Is the student writing an answer on the whiteboard as part of a group activity or individual response?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: Is the student writing on the whiteboard part of a group activity or working individually?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Is the student writing an answer on the whiteboard as part of a group activity or individual response?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[2] Agent Response: The context does not specify whether the student writing on the whiteboard is doing so as part of a group activity or providing an individual response.\n",
      "[3] Question: Are the books on the teacher's desk related to the subject currently being taught or for future lessons?\n",
      "\u001b[1;3;33m> Current query: Are the books on the teacher's desk related to the subject currently being taught or for future lessons?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What subject is currently being taught by the teacher?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Are the books on the teacher's desk related to the subject currently being taught or for future lessons?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[3] Agent Response: The context does not provide information about whether the books on the teacher's desk are related to the subject currently being taught or for future lessons.\n",
      "[4] Question: Is the projector used more frequently for presentations or for showing educational videos?\n",
      "\u001b[1;3;33m> Current query: Is the projector used more frequently for presentations or for showing educational videos?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: How often is the projector used for presentations and educational videos in the classroom?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Is the projector used more frequently for presentations or for showing educational videos?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[4] Agent Response: The projector's usage frequency for presentations and showing educational videos cannot be determined based on the provided context information.\n",
      "[5] Question: How do the dynamics of students working in groups compare to individual work in terms of engagement and productivity?\n",
      "\u001b[1;3;33m> Current query: How do the dynamics of students working in groups compare to individual work in terms of engagement and productivity?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the dynamics of students working in groups in the classroom?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: How do the dynamics of students working in groups compare to individual work in terms of engagement and productivity?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the dynamics of individual student work in the classroom?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: How do the dynamics of students working in groups compare to individual work in terms of engagement and productivity?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[5] Agent Response: The dynamics of students working in groups may differ from individual work in terms of engagement and productivity.\n",
      "[6] Question: Can the student raising their hand to ask a question indicate a high level of classroom participation?\n",
      "\u001b[1;3;33m> Current query: Can the student raising their hand to ask a question indicate a high level of classroom participation?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What behaviors indicate a high level of classroom participation?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Can the student raising their hand to ask a question indicate a high level of classroom participation?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: Does the context mention students raising their hand to ask a question in the classroom?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Can the student raising their hand to ask a question indicate a high level of classroom participation?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[6] Agent Response: Raising their hand to ask a question can indicate a high level of classroom participation.\n",
      "[7] Question: Is the teacher using the map on the wall for a geography lesson or historical context?\n",
      "\u001b[1;3;33m> Current query: Is the teacher using the map on the wall for a geography lesson or historical context?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What is the teacher using the map on the wall for?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Is the teacher using the map on the wall for a geography lesson or historical context?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[7] Agent Response: There is no information provided in the context about the teacher using the map on the wall for either a geography lesson or historical context.\n",
      "[8] Question: Do open windows in the classroom contribute to a better learning environment by improving air quality?\n",
      "\u001b[1;3;33m> Current query: Do open windows in the classroom contribute to a better learning environment by improving air quality?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: Does opening windows in the classroom improve air quality?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Do open windows in the classroom contribute to a better learning environment by improving air quality?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[8] Agent Response: The impact of open windows in a classroom on creating a better learning environment by enhancing air quality is not specified in the context provided.\n",
      "[9] Question: Is the student using their laptop for research, writing, or browsing unrelated content?\n",
      "\u001b[1;3;33m> Current query: Is the student using their laptop for research, writing, or browsing unrelated content?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: What is the student doing on their laptop?\n",
      "\u001b[0m\u001b[1;3;33m> Current query: Is the student using their laptop for research, writing, or browsing unrelated content?\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
      "\u001b[0m[9] Agent Response: The context does not provide information about whether the student is using their laptop for research, writing, or browsing unrelated content.\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for idx, question in enumerate(train_questions[41:]):\n",
    "    print(f\"[{idx}] Question: {question}\")\n",
    "    response = query_engine.query(question)\n",
    "    responses.append(response)\n",
    "    print(f\"[{idx}] Response: {str(response)}\")\n",
    "\n",
    "# question = train_questions[0]\n",
    "# response_gpt4 = query_engine.query(\n",
    "#     \"Is the teacher standing in front of the classroom while giving a lecture or conducting an activity?\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine.callback_manager.end_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m responses:   \n\u001b[1;32m----> 2\u001b[0m     sub_qa \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_qa\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m     tuples \u001b[38;5;241m=\u001b[39m [(t[\u001b[38;5;241m0\u001b[39m], t[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m sub_qa]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tuples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for response in responses:   \n",
    "    sub_qa = response.metadata[\"sub_qa\"]\n",
    "    tuples = [(t[0], t[1]) for t in sub_qa]\n",
    "    if len(tuples) > 0:\n",
    "        latest_tuple = tuples[-1]\n",
    "        latest_query = latest_tuple[0]\n",
    "    print(latest_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 17 examples to finetuning_events_10q.jsonl\n"
     ]
    }
   ],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events_10q.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import OpenAIFinetuneEngine\n",
    "\n",
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"finetuning_events_10q.jsonl\",\n",
    "    #start_job_id=\"<start-job-id>\"  # if you have an existing job, can specify id here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 0\n",
      "First example:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m finetune_engine\u001b[38;5;241m.\u001b[39mfinetune()\n",
      "File \u001b[1;32mc:\\Users\\arnav\\anaconda3\\Lib\\site-packages\\llama_index\\finetuning\\openai\\base.py:62\u001b[0m, in \u001b[0;36mOpenAIFinetuneEngine.finetune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finetune model.\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_json:\n\u001b[1;32m---> 62\u001b[0m     validate_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to specify file name in the new API\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# file_name = os.path.basename(self.data_path)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# upload file\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\arnav\\anaconda3\\Lib\\site-packages\\llama_index\\finetuning\\openai\\validate_json.py:30\u001b[0m, in \u001b[0;36mvalidate_json\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum examples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst example:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Now that we have a sense of the data, we need to go through all the different\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# examples and check to make sure the formatting is correct and matches the Chat\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# completions message structure\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Format error checks\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_llm = finetune_engine.get_finetuned_model(temperature=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
